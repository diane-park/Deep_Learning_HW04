{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diane-park/Deep_Learning_HW04/blob/main/Copy_of_09_Assigment_6_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copyright\n",
        "\n",
        "<PRE>\n",
        "Copyright (c) Bálint Gyires-Tóth - All Rights Reserved\n",
        "You may use and modify this code for research and development purpuses.\n",
        "Using this code for educational purposes (self-paced or instructor led) without the permission of the author is prohibited.\n",
        "</PRE>"
      ],
      "metadata": {
        "id": "CtuSrazlNYEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: RNN text generation with your favorite book\n"
      ],
      "metadata": {
        "id": "vriXNd_nL2q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset\n",
        "- Download your favorite book from https://www.gutenberg.org/\n",
        "- Combine all sonnets into a single text source.  \n",
        "- Split into training (80%) and validation (20%).  "
      ],
      "metadata": {
        "id": "Q5atve1sMH9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: from this link with the entire pride and prejudice text, https://www.gutenberg.org/cache/epub/1342/pg1342.txt\n",
        "\n",
        "import requests\n",
        "\n",
        "# Downloading text from gutenberg website\n",
        "url = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# assign to single string\n",
        "pride_and_prejudice_text = response.text\n",
        "\n",
        "# extract sonnet\n",
        "start_idx = pride_and_prejudice_text.find(\"*** START OF THE PROJECT GUTENBERG EBOOK PRIDE AND PREJUDICE ***\")\n",
        "end_idx = pride_and_prejudice_text.find(\"*** END OF THE PROJECT GUTENBERG EBOOK PRIDE AND PREJUDICE ***\")\n",
        "pride_and_prejudice_text = pride_and_prejudice_text[start_idx:end_idx].strip()\n",
        "\n",
        "train_val_split_idx = int(len(pride_and_prejudice_text)*.8)\n",
        "\n",
        "\n",
        "# test and val split\n",
        "train_data = pride_and_prejudice_text[:train_val_split_idx]\n",
        "val_data = pride_and_prejudice_text[train_val_split_idx:]\n",
        "\n"
      ],
      "metadata": {
        "id": "QvKdt5EyMDug"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing\n",
        "- Convert text to lowercase.  \n",
        "- Remove punctuation (except basic sentence delimiters).  \n",
        "- Tokenize by words or characters (your choice).  \n",
        "- Build a vocabulary (map each unique word to an integer ID)."
      ],
      "metadata": {
        "id": "4eQMcyPgMLJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EbryyuPP3BM",
        "outputId": "6815a68c-5060-4ed8-f075-a7e7d6920ad3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# to lowercase\n",
        "train_data = train_data.lower()\n",
        "val_data = val_data.lower()\n",
        "\n",
        "# use regex to get rid of all unnecessary punctuation and whitespace\n",
        "train_data = re.sub(r\"[^\\w\\s.!?]\", \"\", train_data)\n",
        "val_data = re.sub(r\"[^\\w\\s.!?]\", \"\", val_data)\n",
        "\n",
        "train_tokens = word_tokenize(train_data)\n",
        "val_tokens = word_tokenize(val_data)\n",
        "\n",
        "unique_tokens = set(train_tokens)\n",
        "vocab = {word: idx for idx, word in enumerate(unique_tokens)}\n",
        "unk_token = \"[UNK]\"\n",
        "vocab[unk_token] = len(vocab)\n",
        "\n",
        "train_ids = [vocab[word] for word in train_tokens]\n",
        "\n",
        "val_ids = []\n",
        "\n",
        "for token in val_tokens:\n",
        "    if token in vocab:\n",
        "        val_ids.append(vocab[token])\n",
        "    else:\n",
        "        val_ids.append(vocab[\"[UNK]\"])\n"
      ],
      "metadata": {
        "id": "RvXRFVcbMLe9"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_X = []\n",
        "train_y = []\n",
        "\n",
        "for i in range(len(train_ids) - 3):\n",
        "    train_X.append(train_ids[i:i+3])       # 5-word input\n",
        "    train_y.append(train_ids[i+3])\n",
        "\n",
        "val_X = []\n",
        "val_y = []\n",
        "\n",
        "for i in range(len(val_ids) - 3):\n",
        "    val_X.append(val_ids[i:i+3])       # 5-word input\n",
        "    val_y.append(val_ids[i+3])\n",
        "\n",
        "train_X = np.array(train_X)\n",
        "train_y = np.array(train_y)\n",
        "val_X = np.array(val_X)\n",
        "val_y = np.array(val_y)\n",
        "\n",
        "print(train_X.shape)\n",
        "print(train_y.shape)\n",
        "print(val_X.shape)\n",
        "print(val_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83d4W4GVE439",
        "outputId": "4690a919-ed33-4579-8d02-876070cee865"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(106174, 3)\n",
            "(106174,)\n",
            "(27360, 3)\n",
            "(27360,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Embedding Layer in Keras\n",
        "Below is a minimal example of defining an `Embedding` layer:\n",
        "```python\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,     # size of the vocabulary\n",
        "    output_dim=128,           # embedding vector dimension\n",
        "    input_length=sequence_length\n",
        ")\n",
        "```\n",
        "- This layer transforms integer-encoded sequences (word IDs) into dense vector embeddings.\n",
        "\n",
        "- Feed these embeddings into your LSTM or GRU OR 1D CNN layer."
      ],
      "metadata": {
        "id": "jbTZs3OiMMNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim= len(vocab),     # size of the vocabulary\n",
        "    output_dim= 256,           # embedding vector dimension\n",
        "    input_length= 3\n",
        ")"
      ],
      "metadata": {
        "id": "OXCK40l6MRld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ab1162-6e98-4931-8a02-774d3fc6fc0e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model\n",
        "- Implement an LSTM or GRU or 1D CNN-based language model with:\n",
        "  - **The Embedding layer** as input.\n",
        "  - At least **one recurrent layer** (e.g., `LSTM(256)` or `GRU(256)` or your custom 1D CNN).\n",
        "  - A **Dense** output layer with **softmax** activation for word prediction.\n",
        "- Train for about **5–10 epochs** so it can finish in approximately **2 hours** on a standard machine.\n"
      ],
      "metadata": {
        "id": "qsXR4RZpMXMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# implement early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(GRU(256))\n",
        "model.add(Dropout(0.5))\n",
        "# model.add(LSTM(256))\n",
        "# model.add(Dropout(0.3))\n",
        "model.add(Dense(len(vocab), activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "linweGaUMg0T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "e81be7a4-0725-4616-9fdf-cfc81ec5d617"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_3 (\u001b[38;5;33mGRU\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training & Evaluation\n",
        "- **Monitor** the loss on both training and validation sets.\n",
        "- **Perplexity**: a common metric for language models.\n",
        "  - It is the exponent of the average negative log-likelihood.\n",
        "  - If your model outputs cross-entropy loss `H`, then `perplexity = e^H`.\n",
        "  - Try to keep the validation perplexity **under 50** if possible."
      ],
      "metadata": {
        "id": "Ggop4h4IMhMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_X, train_y, validation_data= (val_X, val_y), epochs=10, batch_size=64, callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "P8d8FS2XMj46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b076feb-e7da-4bf5-8e81-cfba2bbe59d4"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1659/1659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 103ms/step - accuracy: 0.0543 - loss: 6.5251 - val_accuracy: 0.1186 - val_loss: 5.4713\n",
            "Epoch 2/10\n",
            "\u001b[1m1659/1659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 101ms/step - accuracy: 0.1258 - loss: 5.4451 - val_accuracy: 0.1350 - val_loss: 5.2597\n",
            "Epoch 3/10\n",
            "\u001b[1m1659/1659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 99ms/step - accuracy: 0.1544 - loss: 5.1063 - val_accuracy: 0.1455 - val_loss: 5.1894\n",
            "Epoch 4/10\n",
            "\u001b[1m1659/1659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 99ms/step - accuracy: 0.1762 - loss: 4.8267 - val_accuracy: 0.1515 - val_loss: 5.1736\n",
            "Epoch 5/10\n",
            "\u001b[1m1659/1659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 101ms/step - accuracy: 0.1938 - loss: 4.6014 - val_accuracy: 0.1480 - val_loss: 5.1944\n",
            "Epoch 6/10\n",
            "\u001b[1m1659/1659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 101ms/step - accuracy: 0.2145 - loss: 4.3429 - val_accuracy: 0.1514 - val_loss: 5.2379\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7995925b0910>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_acc = model.evaluate(val_X, val_y)\n",
        "\n",
        "print(\"Val Perplexity: \", np.exp(val_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY2Ykaf3-LA2",
        "outputId": "4b4bb0de-000b-4b0f-e037-180ae777e51b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m855/855\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.1510 - loss: 5.1841\n",
            "Val Perplexity:  176.54989624584874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Generation Criteria\n",
        "- After training, generate **two distinct text samples**, each at least **50 tokens**.\n",
        "- Use **different seed phrases** (e.g., “love is” vs. “time will”)."
      ],
      "metadata": {
        "id": "cbvbBOp3MfTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a function to generate text for a seed phrase\n",
        "def generate_text(seed_phrase, model, vocab, min_tokens=50):\n",
        "    seed_tokens = word_tokenize(seed_phrase.lower())\n",
        "    seed_ids = [vocab[word] if word in vocab else vocab[\"[PAD]\"] for word in seed_tokens]\n",
        "    if len(seed_ids) < 3:\n",
        "        seed_ids = [vocab[\"[UNK]\"]] * (3 - len(seed_ids)) + seed_ids\n",
        "    generated_tokens = seed_tokens.copy()\n",
        "\n",
        "    count = 0\n",
        "    if count < min_tokens:\n",
        "        while len(generated_tokens) < min_tokens:\n",
        "            input_sequence = seed_ids[-3:]\n",
        "            input_sequence = np.array([input_sequence])\n",
        "            predicted_probs = model.predict(input_sequence,verbose=0)[0]\n",
        "            predicted_word_idx = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
        "            predicted_word = list(vocab.keys())[list(vocab.values()).index(predicted_word_idx)]\n",
        "            generated_tokens.append(predicted_word)\n",
        "            seed_ids.append(predicted_word_idx)\n",
        "            count += 1\n",
        "\n",
        "    generated_text = \" \".join(generated_tokens)\n",
        "    return generated_text\n",
        "\n"
      ],
      "metadata": {
        "id": "1uHjn6aHMW5K"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"love is\", model, vocab)"
      ],
      "metadata": {
        "id": "n5CpdqF9MoPj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a7dac392-c84d-4b5a-e283-02953854eda2"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'love is done my dear she saw your marrying so disgracing him known nor his collection made her remembered her husband . for he do said to only have and allowed him to believe the appearance of being manner among publicly matrimony ten weeks from my aunt . dear lydia'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"time will\", model, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "1grXTs5Q8m8H",
        "outputId": "6eb18fb6-55dc-48b4-e056-40db236d7426"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time will at present i can have actually hearty houses on goodhumour to their own family . _that_ state to prevent their pretended from colonel fitzwilliam then perceptible i am exceedingly ? but it she can a great sooner one could ever be end to ah mr. wickham to herself'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0_0WhNkP9Yfp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}